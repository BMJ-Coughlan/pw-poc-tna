name: Playwright Tests

# Automated CI/CD pipeline with intelligent test selection
# - Auto-triggers on push/PR to main and develop branches
# - Supports manual execution with custom test filtering
# - Dynamically selects test strategy based on branch context
on:
  push:
    branches:
      - develop
      - main
  pull_request:
    branches:
      - develop
      - main

  workflow_dispatch:
    inputs:
      tags:
        description: 'Test tags to run (e.g., @smoke, @regression, or leave empty for auto-select based on branch)'
        required: false
        type: string
        default: ''
      environment:
        description: 'Environment configuration'
        required: false
        type: choice
        options:
          - auto
          - production
          - staging
          - development
        default: 'auto'

permissions:
  contents: read
  checks: write
  pull-requests: write

jobs:
  test:
    name: Run Playwright Tests
    timeout-minutes: 15
    runs-on: ubuntu-latest

    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, firefox, webkit]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      # Intelligent test selection: balances speed vs coverage based on branch context
      # - main: full regression (high confidence before production)
      # - develop: complete suite (comprehensive integration validation)
      # - PRs/features: smoke tests only (fast feedback for code review)
      # - Manual runs can override with custom tags
      - name: Determine test strategy
        id: test-strategy
        run: |
          if [ -n "${{ inputs.tags }}" ]; then
            echo "tags=${{ inputs.tags }}" >> $GITHUB_OUTPUT
            echo "strategy=manual" >> $GITHUB_OUTPUT
          elif [ "${{ github.ref }}" == "refs/heads/main" ]; then
            echo "tags=@regression" >> $GITHUB_OUTPUT
            echo "strategy=regression" >> $GITHUB_OUTPUT
          elif [ "${{ github.ref }}" == "refs/heads/develop" ]; then
            echo "tags=" >> $GITHUB_OUTPUT
            echo "strategy=full" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "tags=@smoke" >> $GITHUB_OUTPUT
            echo "strategy=smoke" >> $GITHUB_OUTPUT
          else
            echo "tags=@smoke" >> $GITHUB_OUTPUT
            echo "strategy=smoke" >> $GITHUB_OUTPUT
          fi

      # Environment selection using prefix-based variable groups
      # Automatically maps branches to environments or allows manual override
      # Variables are prefixed (e.g., production_BASE_URL, staging_BASE_URL)
      - name: Determine environment configuration
        id: env-config
        run: |
          if [ "${{ inputs.environment }}" == "production" ]; then
            echo "env_name=production" >> $GITHUB_OUTPUT
          elif [ "${{ inputs.environment }}" == "staging" ]; then
            echo "env_name=staging" >> $GITHUB_OUTPUT
          elif [ "${{ inputs.environment }}" == "development" ]; then
            echo "env_name=development" >> $GITHUB_OUTPUT
          else
            if [ "${{ github.ref }}" == "refs/heads/main" ]; then
              echo "env_name=production" >> $GITHUB_OUTPUT
            elif [ "${{ github.ref }}" == "refs/heads/develop" ]; then
              echo "env_name=staging" >> $GITHUB_OUTPUT
            else
              echo "env_name=development" >> $GITHUB_OUTPUT
            fi
          fi

      - name: Display test configuration
        run: |
          echo "ðŸŽ¯ Test Strategy: ${{ steps.test-strategy.outputs.strategy }}"
          echo "ðŸ·ï¸  Test Tags: ${{ steps.test-strategy.outputs.tags || 'all tests' }}"
          echo "ðŸŒ Environment: ${{ steps.env-config.outputs.env_name }}"

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps ${{ matrix.browser }}

      - name: Run TypeScript type checking
        run: npm run typecheck

      - name: Run ESLint
        run: npm run lint

      - name: Run Playwright tests
        run: |
          if [ -n "${{ steps.test-strategy.outputs.tags }}" ]; then
            npx playwright test --project=${{ matrix.browser }} --grep "${{ steps.test-strategy.outputs.tags }}"
          else
            npx playwright test --project=${{ matrix.browser }}
          fi
        env:
          CI: true
          # Dynamic variable resolution: uses format() to select environment-prefixed variables
          # e.g., production_BASE_URL, staging_BASE_URL, development_BASE_URL
          BASE_URL: ${{ vars[format('{0}_BASE_URL', steps.env-config.outputs.env_name)] }}
          COOKIE_DOMAIN: ${{ vars[format('{0}_COOKIE_DOMAIN', steps.env-config.outputs.env_name)] }}
          # Sensitive credentials stored in GitHub Secrets (encrypted at rest)
          E2E_TEST_PASSWORD: ${{ secrets.E2E_TEST_PASSWORD }}
          E2E_WRONG_PASSWORD: ${{ secrets.E2E_WRONG_PASSWORD }}
          E2E_CORRECT_PASSWORD: ${{ secrets.E2E_CORRECT_PASSWORD }}
          API_TEST_PASSWORD: ${{ secrets.API_TEST_PASSWORD }}
          # Non-sensitive test data stored in GitHub Variables (plain text)
          E2E_INVALID_USERNAME: ${{ vars.E2E_INVALID_USERNAME }}
          E2E_VALIDATION_USERNAME: ${{ vars.E2E_VALIDATION_USERNAME }}

      # Interactive test reporting: integrates JUnit XML with GitHub Checks UI
      # Provides pass rates, trends, and detailed failure analysis in the Checks tab
      - name: Publish Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            test-results/junit.xml
          check_name: 'Test Results (${{ matrix.browser }})'
          comment_mode: off
          github_token: ${{ secrets.GITHUB_TOKEN }}

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-report-${{ matrix.browser }}
          path: playwright-report/
          retention-days: 7

      - name: Upload JUnit XML
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: junit-results-${{ matrix.browser }}
          path: test-results/junit.xml
          retention-days: 7

      - name: Upload test traces
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: playwright-traces-${{ matrix.browser }}
          path: test-results/
          retention-days: 7

  # Aggregate job to check if all matrix jobs passed
  test-results:
    name: Test Results Summary
    runs-on: ubuntu-latest
    needs: test
    if: always()
    permissions:
      checks: write
      pull-requests: write

    steps:
      - name: Download all JUnit results
        uses: actions/download-artifact@v4
        with:
          path: all-junit-results/
          pattern: junit-results-*

      - name: Publish Combined Test Results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: |
            all-junit-results/**/*.xml
          check_name: 'Combined Test Results (All Browsers)'
          comment_title: 'Test Results Summary'
          compare_to_earlier_commit: true

      - name: Check test matrix results
        run: |
          if [ "${{ needs.test.result }}" != "success" ]; then
            echo "âŒ Tests failed in one or more browsers"
            exit 1
          else
            echo "âœ… All tests passed across all browsers"
          fi

      - name: Download all reports
        uses: actions/download-artifact@v4
        with:
          path: all-reports/

      - name: List artifacts
        run: ls -R all-reports/

  # Visual Regression Job - DISABLED/QUARANTINED
  # These tests are unreliable due to third-party Google Ads on the practice site
  # causing pixel differences between runs. Uncomment when testing against
  # an ad-free environment or using dedicated visual platforms (Percy.io, Chromatic).
  #
  # visual:
  #   name: Visual Regression (Chromium on Windows)
  #   runs-on: windows-latest
  #   timeout-minutes: 20
  #   if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
  #
  #   steps:
  #     - name: Checkout code
  #       uses: actions/checkout@v4
  #
  #     - name: Setup Node.js
  #       uses: actions/setup-node@v4
  #       with:
  #         node-version: '20'
  #         cache: 'npm'
  #
  #     - name: Install dependencies
  #       run: npm ci
  #
  #     - name: Install Playwright Chromium
  #       run: npx playwright install --with-deps chromium
  #
  #     - name: Determine environment configuration
  #       id: env-config
  #       shell: bash
  #       run: |
  #         if [ "${{ github.ref }}" == "refs/heads/main" ]; then
  #           echo "env_name=production" >> $GITHUB_OUTPUT
  #         elif [ "${{ github.ref }}" == "refs/heads/develop" ]; then
  #           echo "env_name=staging" >> $GITHUB_OUTPUT
  #         else
  #           echo "env_name=development" >> $GITHUB_OUTPUT
  #         fi
  #
  #     - name: Run visual tests (@visual)
  #       run: |
  #         npx playwright test --project=chromium --grep @visual
  #       env:
  #         CI: true
  #         BASE_URL: ${{ vars[format('{0}_BASE_URL', steps.env-config.outputs.env_name)] }}
  #         COOKIE_DOMAIN: ${{ vars[format('{0}_COOKIE_DOMAIN', steps.env-config.outputs.env_name)] }}
  #         E2E_TEST_PASSWORD: ${{ secrets.E2E_TEST_PASSWORD }}
  #         E2E_WRONG_PASSWORD: ${{ secrets.E2E_WRONG_PASSWORD }}
  #         E2E_CORRECT_PASSWORD: ${{ secrets.E2E_CORRECT_PASSWORD }}
  #         API_TEST_PASSWORD: ${{ secrets.API_TEST_PASSWORD }}
  #         E2E_INVALID_USERNAME: ${{ vars.E2E_INVALID_USERNAME }}
  #         E2E_VALIDATION_USERNAME: ${{ vars.E2E_VALIDATION_USERNAME }}
  #
  #     - name: Upload visual report
  #       uses: actions/upload-artifact@v4
  #       if: always()
  #       with:
  #         name: playwright-visual-report
  #         path: playwright-report/
  #         retention-days: 7
  #
  #     - name: Upload visual diffs
  #       uses: actions/upload-artifact@v4
  #       if: failure()
  #       with:
  #         name: visual-diffs
  #         path: |
  #           test-results/**/*-diff.png
  #           test-results/**/*-actual.png
  #           test-results/**/*-expected.png
  #         retention-days: 7

